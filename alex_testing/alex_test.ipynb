{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import cv2\n",
    "\n",
    "def interpolate_patch(image, corrupted_idx, method='linear'):\n",
    "    h, w = image.shape\n",
    "    i, j, dh, dw = corrupted_idx\n",
    "    \n",
    "    # Points to use for interpolation - the boundaries surrounding the corrupted patch\n",
    "    mask = np.ones_like(image, dtype=bool)\n",
    "    mask[i:i+dh, j:j+dw] = False\n",
    "    y, x = np.nonzero(mask)\n",
    "    points = np.array(list(zip(y, x)))\n",
    "    values = image[y, x]\n",
    "    \n",
    "    # Points to interpolate\n",
    "    grid_y, grid_x = np.mgrid[i:i+dh, j:j+dw]\n",
    "    grid_points = np.array(list(zip(grid_y.ravel(), grid_x.ravel())))\n",
    "    \n",
    "    # Interpolation\n",
    "    interpolated_values = griddata(points, values, grid_points, method=method).reshape(dh, dw)\n",
    "    \n",
    "    return interpolated_values\n",
    "\n",
    "def reconstruct_image(image, corrupted_idx):\n",
    "    h, w = image.shape\n",
    "    i, j, dh, dw = corrupted_idx\n",
    "    \n",
    "    # Copy original image and prepare the plot\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    reconstructed = image.copy()\n",
    "    axs[0].imshow(image, cmap='gray')\n",
    "    axs[0].set_title('Original Corrupted')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Interpolate from each non-corrupted part\n",
    "    methods = ['nearest', 'linear', 'cubic']\n",
    "    for k, method in enumerate(methods, 1):\n",
    "        patch = interpolate_patch(image, corrupted_idx, method=method)\n",
    "        reconstructed[i:i+dh, j:j+dw] = patch\n",
    "        axs[k].imshow(patch, cmap='gray')\n",
    "        axs[k].set_title(f'Patch via {method}')\n",
    "        axs[k].axis('off')\n",
    "\n",
    "    # Show reconstructed image\n",
    "    reconstructed[i:i+dh, j:j+dw] = np.mean([interpolate_patch(image, corrupted_idx, method=m) for m in methods], axis=0)\n",
    "    axs[4].imshow(reconstructed, cmap='gray')\n",
    "    axs[4].set_title('Reconstructed Image')\n",
    "    axs[4].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    return reconstructed\n",
    "\n",
    "# Simulate an image with a corrupted patch\n",
    "h, w = 100, 100\n",
    "image = cv2.imread('lenna.png', cv2.IMREAD_GRAYSCALE).astype(float)\n",
    "i, j, dh, dw = 30, 30, 20, 20  # Corrupted patch coordinates\n",
    "image[i:i+dh, j:j+dw] = np.nan  # Assume NaN represents corruption\n",
    "\n",
    "# Reconstruct the image\n",
    "corrupted_idx = (i, j, dh, dw)\n",
    "reconstructed = reconstruct_image(image, corrupted_idx)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import numpy.fft as fft\n",
    "\n",
    "def manual_inverse_fft_2d(F):\n",
    "    N, M = F.shape\n",
    "    n = np.arange(N)\n",
    "    m = np.arange(M)\n",
    "    k = n.reshape((N, 1))\n",
    "    l = m.reshape((M, 1))\n",
    "    # Create the 2D frequency bases for rows and columns\n",
    "    exponent_row = np.exp(2j * np.pi * k * n / N)\n",
    "    exponent_col = np.exp(2j * np.pi * l * m / M)\n",
    "    # Apply IFFT by rows and then by columns\n",
    "    intermediate = np.dot(exponent_row, F) / N\n",
    "    return np.dot(intermediate, exponent_col.T) / M  # Note the transpose on exponent_col\n",
    "\n",
    "# Load the image\n",
    "image = imread('lenna.png')\n",
    "if image.ndim == 3:\n",
    "    image = image[:,:,0]  # Convert to grayscale if it's a color image\n",
    "\n",
    "# Compute the FFT using numpy's built-in function\n",
    "F = fft.fft2(image)\n",
    "\n",
    "# Compute the inverse FFT manually\n",
    "reconstructed_image = manual_inverse_fft_2d(F)\n",
    "\n",
    "# Display the original and reconstructed images\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(np.log(np.abs(fft.fftshift(F)) + 1), cmap='gray')\n",
    "plt.title('Magnitude Spectrum')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(np.real(reconstructed_image), cmap='gray')\n",
    "plt.title('Reconstructed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from darts.models import FFT\n",
    "import cv2\n",
    "from darts.timeseries import TimeSeries\n",
    "\n",
    "img = cv2.imread('lenna.png', cv2.IMREAD_GRAYSCALE)\n",
    "img = TimeSeries.from_values(img)\n",
    "model = FFT(required_matches=set(), nr_freqs_to_keep=None)\n",
    "model.fit(img)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from darts.models import FFT\n",
    "from darts.timeseries import TimeSeries\n",
    "from typing import Optional\n",
    "import cv2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "class FFT2D:\n",
    "    def __init__(self, nr_freqs_to_keep: Optional[int] = 10,\n",
    "        required_matches: Optional[set] = None,\n",
    "        trend: Optional[str] = None,\n",
    "        trend_poly_degree: int = 3,):\n",
    "        \"\"\"\n",
    "        Initialize the FFT model for images.\n",
    "        \n",
    "        Parameters:\n",
    "        - nr_freqs_to_keep (int): Number of highest magnitude frequencies to keep.\n",
    "        \"\"\"\n",
    "        self.nr_freqs_to_keep = nr_freqs_to_keep\n",
    "        self.required_matches = required_matches\n",
    "        self.trend = trend\n",
    "        self.trend_poly_degree = trend_poly_degree\n",
    "\n",
    "    def fit(self, image: np.ndarray):\n",
    "        \"\"\"\n",
    "        Apply 2D FFT to the image and keep only the largest frequencies as specified by nr_freqs_to_keep.\n",
    "        \n",
    "        Parameters:\n",
    "        - image (np.array): 2D array representing the grayscale image.\n",
    "        \"\"\"\n",
    "        self.image = image\n",
    "        self._models = [FFT(required_matches=self.required_matches, trend=self.trend, trend_poly_degree=self.trend_poly_degree) for _ in range(image.shape[0])]\n",
    "\n",
    "        for i in range(image.shape[0]):\n",
    "            self._models[i].fit(TimeSeries.from_values(image[i, :]))  # Reverse the horizontal axis if needed\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, n: int, num_samples: int = 1, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Predict the next n values in the time series.\n",
    "        \n",
    "        Parameters:\n",
    "        - n (int): Number of time steps to predict.\n",
    "        - num_samples (int): Number of samples to generate.\n",
    "        - verbose (bool): Whether to print the progress of the prediction.\n",
    "        - horizontal (bool): Whether to predict horizontally or vertically.\n",
    "        \n",
    "        Returns:\n",
    "        - (np.array): Predicted values.\n",
    "        \"\"\"\n",
    "        return np.hstack([self._models[i].predict(n, num_samples, verbose).data_array() for i in range(self.image.shape[0])]).astype(int).reshape(self.image.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "class FFTRepair:\n",
    "    def __init__(self, image:np.ndarray, weight_func:str='equal'):\n",
    "        \"\"\"\n",
    "        Initialize the FFTRepair model.\n",
    "        \n",
    "        Parameters:\n",
    "        - image (np.array): 2D array representing the grayscale image.\n",
    "        - weight_func (str): The weight function to use for combining the predictions.\n",
    "        \"\"\"\n",
    "        self.image = image\n",
    "        self.weight_func = weight_func\n",
    "\n",
    "    def fit(self, index, width, height, nr_freqs_to_keep: Optional[int] = 10, required_matches: Optional[set] = None, trend: Optional[str] = None, trend_poly_degree: int = 3):\n",
    "        \"\"\"\n",
    "        Fit the model to the corrupted patch.\n",
    "\n",
    "        Parameters:\n",
    "        - index (tuple): The top-left corner of the corrupted patch.\n",
    "        - width (int): The width of the corrupted patch.\n",
    "        - height (int): The height of the corrupted patch.\n",
    "        - nr_freqs_to_keep (int): Number of highest magnitude frequencies to keep.\n",
    "        - required_matches (set): Set of frequencies to keep.\n",
    "        - trend (str): The trend to remove from the time series.\n",
    "        - trend_poly_degree (int): The degree of the polynomial to fit the trend.\n",
    "        \"\"\"\n",
    "        left = self.image[index[0]:index[0]+height, max(0,index[1]-width):index[1]]\n",
    "        right = self.image[index[0]:index[0]+height, index[1]+width:min(index[1]+2*width, self.image.shape[1])]\n",
    "        top = self.image[max(0, index[0]-height):index[0], index[1]:index[1]+width]\n",
    "        bottom = self.image[index[0]+height:min(index[0]+2*height,self.image.shape[0]), index[1]:index[1]+width]\n",
    "\n",
    "        self.left_model = FFT2D(nr_freqs_to_keep, required_matches, trend, trend_poly_degree).fit(left)\n",
    "        self.right_model = FFT2D(nr_freqs_to_keep, required_matches, trend, trend_poly_degree).fit(np.flip(right, axis=1)) # Reverse the horizontal axis\n",
    "        self.top_model = FFT2D(nr_freqs_to_keep, required_matches, trend, trend_poly_degree).fit(np.transpose(top)) # Transpose the array\n",
    "        self.bottom_model = FFT2D(nr_freqs_to_keep, required_matches, trend, trend_poly_degree).fit(np.rot90(bottom, -1)) # Rotate the array 90 degrees clockwise\n",
    "\n",
    "        self.part_height = height\n",
    "        self.part_width = width\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, num_samples: int = 1, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Predict the missing patch.\n",
    "        \n",
    "        Parameters:\n",
    "        - num_samples (int): Number of samples to generate.\n",
    "        - verbose (bool): Whether to print the progress of the prediction.\n",
    "        \"\"\"\n",
    "\n",
    "        left = self.left_model.predict(self.part_width, num_samples, verbose)\n",
    "        right = np.flip(self.right_model.predict(self.part_height, num_samples, verbose),axis=1) # Reverse the horizontal axis\n",
    "        top = np.transpose(self.top_model.predict(self.part_height, num_samples, verbose)) # Transpose the array\n",
    "        bottom = np.rot90(self.bottom_model.predict(self.part_height, num_samples, verbose),1) # Rotate the array 90 degrees counterclockwise\n",
    "\n",
    "        # Compute distance-based weights\n",
    "        vertical_weights = np.linspace(1, 1e-3, self.part_height).reshape(-1, 1)\n",
    "        horizontal_weights = np.linspace(1, 1e-3, self.part_width).reshape(1, -1)\n",
    "\n",
    "        # Apply weights to predictions\n",
    "        weighted_left = left * horizontal_weights\n",
    "        weighted_right = right * np.flip(horizontal_weights, axis=1)\n",
    "        weighted_top = top * vertical_weights\n",
    "        weighted_bottom = bottom * np.flip(vertical_weights, axis=0)\n",
    "\n",
    "        if self.weight_func == 'equal':\n",
    "            combined_prediction = (left + right + top + bottom) / 4\n",
    "        elif self.weight_func == 'distance':\n",
    "            weighted_sum = (weighted_left + weighted_right + weighted_top + weighted_bottom)\n",
    "            horizontal_weights_full = np.tile(horizontal_weights, (self.part_height, 1))\n",
    "            vertical_weights_full = np.tile(vertical_weights, (1, self.part_width))\n",
    "\n",
    "            weights_sum = horizontal_weights_full + np.flip(horizontal_weights_full, axis=1) + vertical_weights_full + np.flip(vertical_weights_full, axis=0)\n",
    "\n",
    "            # weights_sum = (horizontal_weights + horizontal_weights + vertical_weights + vertical_weights)\n",
    "            combined_prediction = weighted_sum / weights_sum\n",
    "        else:\n",
    "            raise ValueError('Invalid weight function')\n",
    "\n",
    "        return combined_prediction.astype(int)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "index = (100,50)\n",
    "width, height = 20, 20\n",
    "img = cv2.imread('lenna.png', cv2.IMREAD_GRAYSCALE)\n",
    "corrupted = img[index[0]:index[0]+height,index[1]:index[1]+width]\n",
    "model = FFTRepair(img, weight_func='distance').fit(index, width, height, nr_freqs_to_keep=20, required_matches=None, trend=None, trend_poly_degree=3)\n",
    "predicted= model.predict()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "source": [
    "cpy_img = img.copy()\n",
    "cpy_img[index[0]:index[0]+height,index[1]:index[1]+width] = predicted.reshape(height,width)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# plt.subplot(1, 4, 2)\n",
    "# plt.imshow(corrupted, cmap='gray')\n",
    "# plt.title('Corrupted Part')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.subplot(1, 4, 3)\n",
    "# plt.imshow(predicted, cmap='gray')\n",
    "# plt.title('Predicted Part')\n",
    "# plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(cpy_img, cmap='gray')\n",
    "plt.title('Predicted Part')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "from darts.models.forecasting.forecasting_model import GlobalForecastingModel\n",
    "from darts.timeseries import TimeSeries\n",
    "from typing import Optional, Callable, Sequence, Union, Tuple, List\n",
    "import numpy as np\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts.models.forecasting.fft import _find_relevant_timestamp_attributes, _crop_to_match_seasons\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "source": [
    "class FFT2(GlobalForecastingModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nr_freqs_to_keep: Optional[int] = 10,\n",
    "        required_matches: Optional[set] = None,\n",
    "        trend: Optional[str] = None,\n",
    "        trend_poly_degree: int = 3,\n",
    "    ):\n",
    "        \"\"\"Fast Fourier Transform Model\n",
    "\n",
    "        This model performs forecasting on a TimeSeries instance using FFT, subsequent frequency filtering\n",
    "        (controlled by the `nr_freqs_to_keep` argument) and  inverse FFT, combined with the option to detrend\n",
    "        the data (controlled by the `trend` argument) and to crop the training sequence to full seasonal periods\n",
    "        Note that if the training series contains any NaNs (missing values), these will be filled using\n",
    "        :func:`darts.utils.missing_values.fill_missing_values()`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nr_freqs_to_keep\n",
    "            The total number of frequencies that will be used for forecasting.\n",
    "        required_matches\n",
    "            The attributes of pd.Timestamp that will be used to create a training sequence that is cropped at the\n",
    "            beginning such that the first timestamp of the training sequence and the first prediction point have\n",
    "            matching phases. If the series has a yearly seasonality, include `month`, if it has a monthly\n",
    "            seasonality, include `day`, etc. If not set, or explicitly set to None, the model tries to find the\n",
    "            pd.Timestamp attributes that are relevant for the seasonality automatically.\n",
    "        trend\n",
    "            If set, indicates what kind of detrending will be applied before performing DFT.\n",
    "            Possible values: 'poly', 'exp' or None, for polynomial trend, exponential trend or no trend, respectively.\n",
    "        trend_poly_degree\n",
    "            The degree of the polynomial that will be used for detrending, if `trend='poly'`.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        Automatically detect the seasonal periods, uses the 10 most significant frequencies for\n",
    "        forecasting and expect no global trend to be present in the data:\n",
    "\n",
    "        >>> FFT(nr_freqs_to_keep=10)\n",
    "\n",
    "        Assume the provided TimeSeries instances will have a monthly seasonality and an exponential\n",
    "        global trend, and do not perform any frequency filtering:\n",
    "\n",
    "        >>> FFT(required_matches={'month'}, trend='exp')\n",
    "\n",
    "        Simple usage example, using one of the dataset available in darts\n",
    "        >>> from darts.datasets import AirPassengersDataset\n",
    "        >>> from darts.models import FFT\n",
    "        >>> series = AirPassengersDataset().load()\n",
    "        >>> # increase the number of frequency and use a polynomial trend of degree 2\n",
    "        >>> model = FFT(\n",
    "        >>>     nr_freqs_to_keep=20,\n",
    "        >>>     trend= \"poly\",\n",
    "        >>>     trend_poly_degree=2\n",
    "        >>> )\n",
    "        >>> model.fit(series)\n",
    "        >>> pred = model.predict(6)\n",
    "        >>> pred.values()\n",
    "        array([[471.79323146],\n",
    "               [494.6381425 ],\n",
    "               [504.5659999 ],\n",
    "               [515.82463265],\n",
    "               [520.59404623],\n",
    "               [547.26720705]])\n",
    "\n",
    "        .. note::\n",
    "            `FFT example notebook <https://unit8co.github.io/darts/examples/03-FFT-examples.html>`_ presents techniques\n",
    "            that can be used to improve the forecasts quality compared to this simple usage example.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nr_freqs_to_keep = nr_freqs_to_keep\n",
    "        self.required_matches = required_matches\n",
    "        self.trend = trend\n",
    "        self.trend_poly_degree = trend_poly_degree\n",
    "\n",
    "    @property\n",
    "    def _model_encoder_settings(\n",
    "        self,\n",
    "    ) -> Tuple[\n",
    "        Optional[int],\n",
    "        Optional[int],\n",
    "        bool,\n",
    "        bool,\n",
    "        Optional[List[int]],\n",
    "        Optional[List[int]],\n",
    "    ]:\n",
    "        return None, None, False, False, None, None\n",
    "\n",
    "    @property\n",
    "    def extreme_lags(\n",
    "        self,\n",
    "    ) -> Tuple[\n",
    "        Optional[int],\n",
    "        Optional[int],\n",
    "        Optional[int],\n",
    "        Optional[int],\n",
    "        Optional[int],\n",
    "        Optional[int],\n",
    "    ]:\n",
    "        # TODO: LocalForecastingModels do not yet handle extreme lags properly. Especially\n",
    "        #  TransferableFutureCovariatesLocalForecastingModel, where there is a difference between fit and predict mode)\n",
    "        #  do not yet. In general, Local models train on the entire series (input=output), different to Global models\n",
    "        #  that use an input to predict an output.\n",
    "        return -self.min_train_series_length, -1, None, None, None, None\n",
    "    \n",
    "    @property\n",
    "    def supports_multivariate(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _exp_trend(self, x) -> Callable:\n",
    "        \"\"\"Helper function, used to make FFT model pickable.\"\"\"\n",
    "        return np.exp(self.trend_coefficients[1]) * np.exp(\n",
    "            self.trend_coefficients[0] * x\n",
    "        )\n",
    "\n",
    "    def _poly_trend(self, trend_coefficients) -> Callable:\n",
    "        \"\"\"Helper function, for consistency with the other trends\"\"\"\n",
    "        return np.poly1d(trend_coefficients)\n",
    "\n",
    "    def _null_trend(self, x) -> Callable:\n",
    "        \"\"\"Helper function, used to make FFT model pickable.\"\"\"\n",
    "        return 0\n",
    "\n",
    "    def fit(self, series: Union[TimeSeries, Sequence[TimeSeries]]):\n",
    "        series = fill_missing_values(series)\n",
    "        super().fit(series)\n",
    "        # self._assert_univariate(series)\n",
    "        series = self.training_series\n",
    "\n",
    "        # determine trend\n",
    "        if self.trend == \"poly\":\n",
    "            self.trend_coefficients = np.polyfit(\n",
    "                range(len(series)), series.univariate_values(), self.trend_poly_degree\n",
    "            )\n",
    "            self.trend_function = self._poly_trend(self.trend_coefficients)\n",
    "        elif self.trend == \"exp\":\n",
    "            self.trend_coefficients = np.polyfit(\n",
    "                range(len(series)), np.log(series.univariate_values()), 1\n",
    "            )\n",
    "            self.trend_function = self._exp_trend\n",
    "        else:\n",
    "            self.trend_coefficients = None\n",
    "            self.trend_function = self._null_trend\n",
    "\n",
    "        # subtract trend\n",
    "        detrended_values = series.values()\n",
    "        # detrended_values = series.univariate_values() - self.trend_function(\n",
    "        #     range(len(series))\n",
    "        # )\n",
    "        detrended_series = TimeSeries.from_times_and_values(\n",
    "            series.time_index, detrended_values\n",
    "        )\n",
    "\n",
    "        # crop training set to match the seasonality of the first prediction point\n",
    "        if self.required_matches is None:\n",
    "            curr_required_matches = _find_relevant_timestamp_attributes(\n",
    "                detrended_series\n",
    "            )\n",
    "        else:\n",
    "            curr_required_matches = self.required_matches\n",
    "        cropped_series = _crop_to_match_seasons(\n",
    "            detrended_series, required_matches=curr_required_matches\n",
    "        )\n",
    "\n",
    "        # perform dft\n",
    "        self.fft_values = np.fft.fft2(cropped_series.values()) #cropped_series.univariate_values())\n",
    "\n",
    "        # get indices of `nr_freqs_to_keep` (if a correct value was provided) frequencies with the highest amplitudes\n",
    "        # by partitioning around the element with sorted index -nr_freqs_to_keep instead of sorting the whole array\n",
    "        first_n = self.nr_freqs_to_keep\n",
    "        if first_n is None or first_n < 1 or first_n > len(self.fft_values.flatten()):\n",
    "            first_n = len(self.fft_values.flatten())\n",
    "        # self.filtered_indices = np.argpartition(abs(self.fft_values), -first_n)[\n",
    "        #     -first_n:\n",
    "        # ]\n",
    "        flat_indices = np.argpartition(abs(self.fft_values).ravel(), -first_n)[-first_n:]\n",
    "        self.filtered_indices = np.unravel_index(flat_indices, self.fft_values.shape)\n",
    "\n",
    "        # set all other values in the frequency domain to 0\n",
    "        self.fft_values_filtered = np.zeros_like(self.fft_values, dtype=np.complex_)\n",
    "        self.fft_values_filtered[self.filtered_indices] = self.fft_values[\n",
    "            self.filtered_indices\n",
    "        ]\n",
    "\n",
    "        # precompute all possible predicted values using inverse dft\n",
    "        self.predicted_values = np.fft.ifft2(self.fft_values_filtered).real\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, n: int, num_samples: int = 1, verbose: bool = False):\n",
    "        super().predict(n, num_samples)\n",
    "        # trend_forecast = np.array(\n",
    "        #     [self.trend_function(i + len(self.training_series)) for i in range(n)]\n",
    "        # )\n",
    "        periodic_forecast = np.array(\n",
    "            [self.predicted_values[i % len(self.predicted_values)] for i in range(n)]\n",
    "        )\n",
    "        return self._build_forecast_series(periodic_forecast).values() # + trend_forecast).values()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "source": [
    "class FFTRepair:\n",
    "    def __init__(self, image:np.ndarray, weight_func:str='equal'):\n",
    "        \"\"\"\n",
    "        Initialize the FFTRepair model.\n",
    "        \n",
    "        Parameters:\n",
    "        - image (np.array): 2D array representing the grayscale image.\n",
    "        - weight_func (str): The weight function to use for combining the predictions.\n",
    "        \"\"\"\n",
    "        self.image = image\n",
    "        self.weight_func = weight_func\n",
    "\n",
    "    def fit(self, index, width, height, nr_freqs_to_keep: Optional[int] = 10, required_matches: Optional[set] = None, trend: Optional[str] = None, trend_poly_degree: int = 3):\n",
    "        \"\"\"\n",
    "        Fit the model to the corrupted patch.\n",
    "\n",
    "        Parameters:\n",
    "        - index (tuple): The top-left corner of the corrupted patch.\n",
    "        - width (int): The width of the corrupted patch.\n",
    "        - height (int): The height of the corrupted patch.\n",
    "        - nr_freqs_to_keep (int): Number of highest magnitude frequencies to keep.\n",
    "        - required_matches (set): Set of frequencies to keep.\n",
    "        - trend (str): The trend to remove from the time series.\n",
    "        - trend_poly_degree (int): The degree of the polynomial to fit the trend.\n",
    "        \"\"\"\n",
    "        left = self.image[index[0]:index[0]+height, 0:index[1]] # self.image[index[0]:index[0]+height, max(0,index[1]-width):index[1]]\n",
    "        right = self.image[index[0]:index[0]+height, index[1]+width:self.image.shape[1]] # self.image[index[0]:index[0]+height, index[1]+width:min(index[1]+2*width, self.image.shape[1])]\n",
    "        top = self.image[0:index[0], index[1]:index[1]+width] # self.image[max(0, index[0]-height):index[0], index[1]:index[1]+width]\n",
    "        bottom = self.image[index[0]+height:self.image.shape[0], index[1]:index[1]+width] # self.image[index[0]+height:min(index[0]+2*height,self.image.shape[0]), index[1]:index[1]+width]\n",
    "\n",
    "        self.left_model = FFT2(nr_freqs_to_keep, required_matches, trend, trend_poly_degree).fit(TimeSeries.from_values(left))\n",
    "        self.right_model = FFT2(nr_freqs_to_keep, required_matches, trend, trend_poly_degree).fit(TimeSeries.from_values(np.flip(right, axis=1))) # Reverse the horizontal axis\n",
    "        self.top_model = FFT2(nr_freqs_to_keep, required_matches, trend, trend_poly_degree).fit(TimeSeries.from_values(np.transpose(top))) # Transpose the array\n",
    "        self.bottom_model = FFT2(nr_freqs_to_keep, required_matches, trend, trend_poly_degree).fit(TimeSeries.from_values(np.rot90(bottom, -1))) # Rotate the array 90 degrees clockwise\n",
    "\n",
    "        self.part_height = height\n",
    "        self.part_width = width\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, num_samples: int = 1, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Predict the missing patch.\n",
    "        \n",
    "        Parameters:\n",
    "        - num_samples (int): Number of samples to generate.\n",
    "        - verbose (bool): Whether to print the progress of the prediction.\n",
    "        \"\"\"\n",
    "\n",
    "        left = self.left_model.predict(self.part_width, num_samples, verbose)\n",
    "        right = np.flip(self.right_model.predict(self.part_height, num_samples, verbose),axis=1) # Reverse the horizontal axis\n",
    "        top = np.transpose(self.top_model.predict(self.part_height, num_samples, verbose)) # Transpose the array\n",
    "        bottom = np.rot90(self.bottom_model.predict(self.part_height, num_samples, verbose),1) # Rotate the array 90 degrees counterclockwise\n",
    "\n",
    "        # Compute distance-based weights\n",
    "        vertical_weights = np.linspace(1, 1e-3, self.part_height).reshape(-1, 1)\n",
    "        horizontal_weights = np.linspace(1, 1e-3, self.part_width).reshape(1, -1)\n",
    "\n",
    "        # Apply weights to predictions\n",
    "        weighted_left = left * horizontal_weights\n",
    "        weighted_right = right * np.flip(horizontal_weights, axis=1)\n",
    "        weighted_top = top * vertical_weights\n",
    "        weighted_bottom = bottom * np.flip(vertical_weights, axis=0)\n",
    "\n",
    "        if self.weight_func == 'equal':\n",
    "            combined_prediction = (left + right + top + bottom) / 4\n",
    "        elif self.weight_func == 'distance':\n",
    "            weighted_sum = (weighted_left + weighted_right + weighted_top + weighted_bottom)\n",
    "            horizontal_weights_full = np.tile(horizontal_weights, (self.part_height, 1))\n",
    "            vertical_weights_full = np.tile(vertical_weights, (1, self.part_width))\n",
    "\n",
    "            weights_sum = horizontal_weights_full + np.flip(horizontal_weights_full, axis=1) + vertical_weights_full + np.flip(vertical_weights_full, axis=0)\n",
    "\n",
    "            # weights_sum = (horizontal_weights + horizontal_weights + vertical_weights + vertical_weights)\n",
    "            combined_prediction = weighted_sum / weights_sum\n",
    "        else:\n",
    "            raise ValueError('Invalid weight function')\n",
    "\n",
    "        return combined_prediction.astype(int)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "    \n",
    "index = (100,50)\n",
    "width, height = 20, 20\n",
    "img = cv2.imread('lenna.png', cv2.IMREAD_GRAYSCALE)\n",
    "corrupted = img[index[0]:index[0]+height,index[1]:index[1]+width]\n",
    "model = FFTRepair(img, weight_func='distance').fit(index, width, height, nr_freqs_to_keep=20, required_matches=None, trend=None, trend_poly_degree=3)\n",
    "predicted= model.predict()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "cpy_img = img.copy()\n",
    "cpy_img[index[0]:index[0]+height,index[1]:index[1]+width] = predicted.reshape(height,width)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(corrupted, cmap='gray')\n",
    "plt.title('Corrupted Part')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(predicted, cmap='gray')\n",
    "plt.title('Predicted Part')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(cpy_img, cmap='gray')\n",
    "plt.title('Predicted Part')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example parameters for the corrupted area (x, y, width, height)\n",
    "corrupted_x, corrupted_y, corrupted_width, corrupted_height = 100, 100, 50, 50\n",
    "image = cv2.imread('lenna.png', cv2.IMREAD_GRAYSCALE)  # Example image; replace with your actual image\n",
    "\n",
    "# Define the four regions\n",
    "left = image[:, :corrupted_x]\n",
    "right = image[:, corrupted_x + corrupted_width:]\n",
    "top = image[:corrupted_y, :]\n",
    "bottom = image[corrupted_y + corrupted_height:, :]\n",
    "\n",
    "regions = [left, right, top, bottom]\n",
    "\n",
    "def pad_arrays_to_same_size(arrays):\n",
    "    # Determine the maximum size in each dimension\n",
    "    max_rows = max(arr.shape[0] for arr in arrays)\n",
    "    max_cols = max(arr.shape[1] for arr in arrays)\n",
    "    \n",
    "    # Pad each array to the max size\n",
    "    padded_arrays = [np.pad(arr, ((0, max_rows - arr.shape[0]), (0, max_cols - arr.shape[1])), mode='constant', constant_values=0) for arr in arrays]\n",
    "    return padded_arrays\n",
    "\n",
    "# Applying FFT on each region\n",
    "fft_results = [np.fft.fft2(region) for region in regions]\n",
    "\n",
    "# Pad FFT results to the same size\n",
    "fft_results_padded = pad_arrays_to_same_size(fft_results)\n",
    "\n",
    "# Compute magnitudes\n",
    "fft_magnitudes = [np.abs(fft) for fft in fft_results_padded]\n",
    "\n",
    "# Sum magnitudes to find common frequencies\n",
    "frequency_sum = np.sum(fft_magnitudes, axis=0)\n",
    "common_frequencies = frequency_sum > np.percentile(frequency_sum, 95)  # Example threshold: top 5%\n",
    "\n",
    "# Example reconstruction code using the average common frequencies (adjust as needed)\n",
    "average_fft = np.mean([fft * (frequency_sum > np.percentile(frequency_sum, 95)) for fft in fft_results_padded], axis=0)\n",
    "reconstructed_full  = np.fft.ifft2(average_fft).real\n",
    "\n",
    "reconstructed_part = reconstructed_full[\n",
    "    :corrupted_height, :corrupted_width\n",
    "]\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.title('Original Image with Corrupted Area')\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.axhline(corrupted_y, color='r')\n",
    "plt.axhline(corrupted_y + corrupted_height, color='r')\n",
    "plt.axvline(corrupted_x, color='r')\n",
    "plt.axvline(corrupted_x + corrupted_width, color='r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Reconstructed Part')\n",
    "plt.imshow(reconstructed_part, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
